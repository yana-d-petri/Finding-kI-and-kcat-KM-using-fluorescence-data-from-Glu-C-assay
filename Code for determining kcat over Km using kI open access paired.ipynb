{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a8058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import curve_fit\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + '/usr/local/texlive/2021/bin/x86_64-linux'\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32935a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('bmh')\n",
    "plt.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1c9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family':'serif','size':30, 'serif': ['computer modern roman']}\n",
    "plt.rc('font',**font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b251f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}'\n",
    "plt.rc('axes', grid=False, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d553b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing four Excel spread sheets with fluorescent data into Python\n",
    "#The experiment here was carried out with 8 concentrations of Opt-2, 25 nM PLE, and 1 uM Glu-C\n",
    "#The imported data represents raw data that has not been processed\n",
    "data_high_conc_rep1 = pandas.read_excel('High-Opt-2-concentration-range-kcat-km-rep-1-25-nM-PLE-1-uM-Glu-C-raw-fluorescence-vs-time.xlsx',engine='openpyxl')\n",
    "data_high_conc_rep2 = pandas.read_excel('High-Opt-2-concentration-range-kcat-km-rep-2-25-nM-PLE-1-uM-Glu-C-raw-fluorescence-vs-time.xlsx',engine='openpyxl')\n",
    "data_low_conc_rep1 = pandas.read_excel('Low-Opt-2-concentration-range-kcat-km-rep-1-25-nM-PLE-1-uM-Glu-C-raw-fluorescence-vs-time.xlsx',engine='openpyxl')\n",
    "data_low_conc_rep2 = pandas.read_excel('Low-Opt-2-concentration-range-kcat-km-rep-2-25-nM-PLE-1-uM-Glu-C-raw-fluorescence-vs-time.xlsx',engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917263c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the columns\n",
    "data_high_conc_rep1_column = data_high_conc_rep1.columns\n",
    "data_high_conc_rep2_column = data_high_conc_rep2.columns\n",
    "data_low_conc_rep1_column = data_low_conc_rep1.columns\n",
    "data_low_conc_rep2_column = data_low_conc_rep2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2147ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing times corresponding to fluorescence data points\n",
    "T_HR_rep1 = (data_high_conc_rep1[data_high_conc_rep1_column[1]].values)[1:]\n",
    "T_HR_rep1 = np.array(T_HR_rep1,dtype='float64')\n",
    "\n",
    "T_HR_rep2 = (data_high_conc_rep2[data_high_conc_rep2_column[1]].values)[1:]\n",
    "T_HR_rep2 = np.array(T_HR_rep2,dtype='float64')\n",
    "\n",
    "T_LR_rep1 = (data_low_conc_rep1[data_low_conc_rep1_column[1]].values)[1:]\n",
    "T_LR_rep1 = np.array(T_LR_rep1,dtype='float64')\n",
    "\n",
    "T_LR_rep2 = (data_low_conc_rep2[data_low_conc_rep2_column[1]].values)[1:]\n",
    "T_LR_rep2 = np.array(T_LR_rep2,dtype='float64')\n",
    "\n",
    "#Defining a list of substrate concentrations used in the experiment\n",
    "scaling = 0.7\n",
    "Sub_conc = [10, 10*scaling, 10*scaling**2, 10*scaling**3, 10*scaling**4,10*scaling**5, 10*scaling**6,10*scaling**7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eee17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing background fluorescence values of the buffer \n",
    "buffer_HR_rep1 = (data_high_conc_rep1[data_high_conc_rep1_column[-3:]].values)[1:].T\n",
    "buffer_LR_rep1 = (data_low_conc_rep1[data_low_conc_rep1_column[-3:]].values)[1:].T\n",
    "buffer_HR_rep2 = (data_high_conc_rep2[data_high_conc_rep2_column[-3:]].values)[1:].T\n",
    "buffer_LR_rep2 = (data_low_conc_rep2[data_low_conc_rep2_column[-3:]].values)[1:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d40691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining an array with dimensions  3 x 8 x T_HR_rep where the first dimension is 3 replicates, the second is \n",
    "#concetration of substrate, and the third is fluorescent values taken every 20 seconds\n",
    "F_PG_rep1 = np.zeros((3,8,len(T_HR_rep1)))\n",
    "F_PG_rep2 = np.zeros((3,8,len(T_HR_rep2)))\n",
    "\n",
    "#\"PG\" corresponds to esterified substrate + PLE + Glu-C, \"P\" to substrate + PLE, and \"G\" to substrate + Glu-C\n",
    "#\"Fmax\" corresponds to fluorescence of Opt + Glu-C\n",
    "#Filling the array with data\n",
    "for a in range(3):\n",
    "    for b in range(4):\n",
    "        F_PG_rep1[a,b,:] = (data_high_conc_rep1[data_high_conc_rep1_column[2+a+3*b]].values)[1:] - buffer_HR_rep1[a,:]\n",
    "        F_PG_rep2[a,b,:] = (data_high_conc_rep2[data_high_conc_rep2_column[2+a+3*b]].values)[1:] - buffer_HR_rep2[a,:]\n",
    "        F_PG_rep1[a,4+b,:] = (data_low_conc_rep1[data_low_conc_rep1_column[2+a+3*b]].values)[1:] - buffer_LR_rep1[a,:]\n",
    "        F_PG_rep2[a,4+b,:] = (data_low_conc_rep2[data_low_conc_rep2_column[2+a+3*b]].values)[1:] - buffer_LR_rep2[a,:]\n",
    "        \n",
    "        \n",
    "F_P_rep1 = np.zeros((3,8,len(T_HR_rep1))) \n",
    "F_P_rep2 = np.zeros((3,8,len(T_HR_rep2)))\n",
    "\n",
    "for a in range(3):\n",
    "    for b in range(4):\n",
    "        F_P_rep1[a,b,:] = (data_high_conc_rep1[data_high_conc_rep1_column[14+a+3*b]].values)[1:] - buffer_HR_rep1[a,:]\n",
    "        F_P_rep2[a,b,:] = (data_high_conc_rep2[data_high_conc_rep2_column[14+a+3*b]].values)[1:] - buffer_HR_rep2[a,:]\n",
    "        F_P_rep1[a,4+b,:] = (data_low_conc_rep1[data_low_conc_rep1_column[14+a+3*b]].values)[1:] - buffer_LR_rep1[a,:]\n",
    "        F_P_rep2[a,4+b,:] = (data_low_conc_rep2[data_low_conc_rep2_column[14+a+3*b]].values)[1:] - buffer_LR_rep2[a,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39d8b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_G_rep1 = np.zeros((3,8,len(T_HR_rep1))) \n",
    "F_G_rep2 = np.zeros((3,8,len(T_HR_rep2)))\n",
    "\n",
    "for a in range(3):\n",
    "    for b in range(4):\n",
    "        F_G_rep1[a,b,:] = (data_high_conc_rep1[data_high_conc_rep1_column[26+a+3*b]].values)[1:] - buffer_HR_rep1[a,:]\n",
    "        F_G_rep2[a,b,:] = (data_high_conc_rep2[data_high_conc_rep2_column[26+a+3*b]].values)[1:] - buffer_HR_rep2[a,:]\n",
    "        F_G_rep1[a,4+b,:] = (data_low_conc_rep1[data_low_conc_rep1_column[26+a+3*b]].values)[1:] - buffer_LR_rep1[a,:]\n",
    "        F_G_rep2[a,4+b,:] = (data_low_conc_rep2[data_low_conc_rep2_column[26+a+3*b]].values)[1:] - buffer_LR_rep2[a,:]   \n",
    "    \n",
    "F_max_rep1 = np.zeros((3,8,len(T_HR_rep1)))\n",
    "F_max_rep2 = np.zeros((3,8,len(T_HR_rep2)))\n",
    "\n",
    "for a in range(3):\n",
    "    for b in range(4):\n",
    "        F_max_rep1[a,b,:] = (data_high_conc_rep1[data_high_conc_rep1_column[38+a+3*b]].values)[1:] - buffer_HR_rep1[a,:]\n",
    "        F_max_rep2[a,b,:] = (data_high_conc_rep2[data_high_conc_rep2_column[38+a+3*b]].values)[1:] - buffer_HR_rep2[a,:]\n",
    "        F_max_rep1[a,4+b,:] = (data_low_conc_rep1[data_low_conc_rep1_column[38+a+3*b]].values)[1:] - buffer_LR_rep1[a,:]\n",
    "        F_max_rep2[a,4+b,:] = (data_low_conc_rep2[data_low_conc_rep2_column[38+a+3*b]].values)[1:] - buffer_LR_rep2[a,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01946c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the average of times across 4 datasets         \n",
    "T = (T_HR_rep1 + T_HR_rep2 + T_LR_rep1 + T_LR_rep2)/4 #assume no error here\n",
    "\n",
    "#Adding 15 seconds to each time point because the measurements started at t = 15 sec (shaking for first 15 s)\n",
    "T = T + 15  \n",
    "    \n",
    "#After defining all columns, it is useful to check that everything has been imported correctly as follows:    \n",
    "#print(F_max_rep2[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a84ce32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Remove commenting out for plotting\\n#The part below is a visual check (plotting Pt_averaged vs time)\\nplt.figure(figsize=(10, 6))\\nfor i in range(Pt_averaged.shape[0]):\\n        plt.plot(T, Pt_averaged[i, :], label=f'{rounded_conc[i]} uM of subst.')\\n        plt.plot(T, Pt_averaged[i, :]+Pt_std[i, :], alpha=0.3)\\n        plt.plot(T, Pt_averaged[i, :]-Pt_std[i, :], alpha=0.3)\\nplt.xlabel('Time (s)')\\nplt.ylabel('[P]t (uM)')\\nplt.title('Pt_Averaged with STD error vs Time')\\nplt.grid(True)\\nplt.legend(fontsize='x-small')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly pairing F_PG, F_G, F_max, and F_P from either from rep1 (and then doing the same thing with rep2)\n",
    "Pt_rep1 = np.zeros((3**4,8,len(T)))\n",
    "Pt_rep2 = np.zeros((3**4,8,len(T)))\n",
    "\n",
    "import itertools\n",
    "for k in range(8):\n",
    "    index = 0\n",
    "    for a,b,c,d in itertools.product(range(3),repeat=4):\n",
    "        Pt_rep1[index,k,:] = Sub_conc[k]*(F_PG_rep1[a,k]-F_G_rep1[b,k])/(F_max_rep1[c,k]-F_P_rep1[d,k])\n",
    "        Pt_rep2[index,k,:] = Sub_conc[k]*(F_PG_rep2[a,k]-F_G_rep2[b,k])/(F_max_rep2[c,k]-F_P_rep2[d,k])\n",
    "        index += 1\n",
    "    \n",
    "#Finding standard deviation of all product concentrations that resulted from random pairings\n",
    "#Also doing the same for rep1\n",
    "std_Pt_rep1 = np.std(Pt_rep1, axis=0)\n",
    "std_Pt_rep2 = np.std(Pt_rep2, axis=0)\n",
    "\n",
    "#Propagating error for finding standard deviation in the two replicates \n",
    "Pt_std = np.sqrt((std_Pt_rep1**2 + std_Pt_rep2**2)/4)\n",
    "    \n",
    "Pt = np.concatenate((Pt_rep1,Pt_rep2),axis=0)\n",
    "\n",
    "#finding the average of all product concentrations that resulted from random pairings\n",
    "Pt_averaged = np.mean(Pt,axis=0)\n",
    "                      \n",
    "#Exporting Pt_averaged values, corresponding STD values, and time values into Excel for plotting \n",
    "Pt_averaged_df = pandas.DataFrame(Pt_averaged)\n",
    "Pt_std_df = pandas.DataFrame(Pt_std)\n",
    "T_df = pandas.DataFrame(T)\n",
    "\n",
    "with pandas.ExcelWriter('OUTPUT_kcat_kM_experiment_Opt-2_averaged_Product_Concentration_std_and_Time.xlsx', engine='openpyxl') as writer:\n",
    "    Pt_averaged_df.to_excel(writer, sheet_name='Pt_averaged')\n",
    "    Pt_std_df.to_excel(writer, sheet_name='Pt_std')\n",
    "    T_df.to_excel(writer, sheet_name='Time')\n",
    "\n",
    "rounded_conc = [round(x, 1) for x in Sub_conc] #rounding concentrations to display in plots\n",
    "\n",
    "'''\n",
    "#Remove commenting out for plotting\n",
    "#The part below is a visual check (plotting Pt_averaged vs time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(Pt_averaged.shape[0]):\n",
    "        plt.plot(T, Pt_averaged[i, :], label=f'{rounded_conc[i]} uM of subst.')\n",
    "        plt.plot(T, Pt_averaged[i, :]+Pt_std[i, :], alpha=0.3)\n",
    "        plt.plot(T, Pt_averaged[i, :]-Pt_std[i, :], alpha=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('[P]t (uM)')\n",
    "plt.title('Pt_Averaged with STD error vs Time')\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1224bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining kI found in previous experiment along with its standard deviation    \n",
    "kI = 0.003185705691107232\n",
    "kI_SD = 6.086274191781751e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47aeb8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Remove commenting out for plotting\\n#The part below is a visual check (plotting Pt_averaged vs time)\\nplt.figure(figsize=(10, 6))\\nfor i in range(Pt_averaged.shape[0]):\\n        plt.plot(T_der, dPdt_averaged[i, :], label=f'{rounded_conc[i]} uM of subst.')\\n        plt.plot(T_der, dPdt_averaged[i, :]+dPdt_std[i, :], alpha=0.3)\\n        plt.plot(T_der, dPdt_averaged[i, :]-dPdt_std[i, :], alpha=0.3)\\nplt.xlabel('Time (s)')\\nplt.ylabel('dPdt (uM s-1)')\\nplt.title('dPdt_Averaged with STD error vs T_der')\\nplt.grid(True)\\nplt.legend(fontsize='x-small')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The formula for At = Pt + kI*(dPdt)\n",
    "#We will first find dPdt to eventually find A\n",
    "\n",
    "#In the manuscript, n = nr + nl\n",
    "n = 4 #Spacing of n between subsequent concentrations was chosen to compute the derivative dPdt\n",
    "#Note that the value of dPdt is not very sensitive to the choice of n\n",
    "\n",
    "#dPdt is easy to plot, but error propagation on dPdt from error on Pt_averaged is challenging\n",
    "#To avoid challenging math/overestimating the error, we will take an alternative appriach:\n",
    "#For each of the 6 replicates (3 technical, 2 indepednent), dPdt will be found (6 values per concentration)\n",
    "#Then, the regular error propagation formula will be applied to the 6 resultant values of dPdt \n",
    "\n",
    "#Finding derivative curves for 81 Pt curves in each of the rep (rep 1 and rep 2)\n",
    "dPdt_rep1 = (Pt_rep1[:,:,n:] - Pt_rep1[:,:,:-n])/(T[n:] - T[:-n]) #Computing dPdt for 81 Pt curves in rep1 \n",
    "dPdt_rep2 = (Pt_rep2[:,:,n:] - Pt_rep2[:,:,:-n])/(T[n:] - T[:-n]) #Computing dPdt for 81 Pt curves in rep2\n",
    "\n",
    "#Finding standard deviations corresponding to the derivative curves\n",
    "std_dPdt_rep1 = np.std(dPdt_rep1, axis=0)\n",
    "std_dPdt_rep2 = np.std(dPdt_rep2, axis=0)\n",
    "\n",
    "#Propagating error for finding standard deviation in the two replicates \n",
    "dPdt_std = np.sqrt((std_dPdt_rep1**2 + std_dPdt_rep2**2)/4)\n",
    "dPdt = np.concatenate((dPdt_rep1,dPdt_rep2),axis=0)\n",
    "\n",
    "#Dinding the average of all derivative curves that resulted from product curves\n",
    "dPdt_averaged = np.mean(dPdt,axis=0)\n",
    "\n",
    "T_der = (T[n:] + T[:-n])/2 #Computing times corresponding to derivative dPdt\n",
    "\n",
    "'''\n",
    "#Remove commenting out for plotting\n",
    "#The part below is a visual check (plotting Pt_averaged vs time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(Pt_averaged.shape[0]):\n",
    "        plt.plot(T_der, dPdt_averaged[i, :], label=f'{rounded_conc[i]} uM of subst.')\n",
    "        plt.plot(T_der, dPdt_averaged[i, :]+dPdt_std[i, :], alpha=0.3)\n",
    "        plt.plot(T_der, dPdt_averaged[i, :]-dPdt_std[i, :], alpha=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('dPdt (uM s-1)')\n",
    "plt.title('dPdt_Averaged with STD error vs T_der')\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff3c966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Remove commenting out for plotting\\n#The part below is a visual check (plotting Pt_averaged vs time)\\nplt.figure(figsize=(10, 6))\\nfor i in range(Pt_averaged.shape[0]):\\n        plt.plot(T_der, P_mid_averaged[i, :], label=f'{rounded_conc[i]} uM of subst.')\\n        plt.plot(T_der, P_mid_averaged[i, :]+P_mid_std[i, :], alpha=0.3)\\n        plt.plot(T_der, P_mid_averaged[i, :]-P_mid_std[i, :], alpha=0.3)\\nplt.xlabel('Time (s)')\\nplt.ylabel('P_mid (uM)')\\nplt.title('P_mid_Averaged with STD error vs T_der')\\nplt.grid(True)\\nplt.legend(fontsize='x-small')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The formula for At is At = Pt + kI*(dPdt) or At = Pt + It\n",
    "#The time corresponding to derivative is in the middle of time points; hence need to find Pt\n",
    "#that would correspond to the time at which the derivative was found\n",
    "\n",
    "#The errors here were computed the same way as for dPdt\n",
    "P_mid_rep1 = (Pt_rep1[:,:,n:] + Pt_rep1[:,:,:-n])/2\n",
    "P_mid_rep2 = (Pt_rep2[:,:,n:] + Pt_rep2[:,:,:-n])/2 \n",
    "\n",
    "std_P_mid_rep1 = np.std(P_mid_rep1, axis=0)\n",
    "std_P_mid_rep2 = np.std(P_mid_rep2, axis=0)\n",
    "\n",
    "P_mid_std = np.sqrt((std_P_mid_rep1**2 + std_P_mid_rep2**2)/4)\n",
    "P_mid = np.concatenate((P_mid_rep1,P_mid_rep2),axis=0)\n",
    "\n",
    "P_mid_averaged = np.mean(P_mid,axis=0)\n",
    "\n",
    "'''\n",
    "#Remove commenting out for plotting\n",
    "#The part below is a visual check (plotting Pt_averaged vs time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(Pt_averaged.shape[0]):\n",
    "        plt.plot(T_der, P_mid_averaged[i, :], label=f'{rounded_conc[i]} uM of subst.')\n",
    "        plt.plot(T_der, P_mid_averaged[i, :]+P_mid_std[i, :], alpha=0.3)\n",
    "        plt.plot(T_der, P_mid_averaged[i, :]-P_mid_std[i, :], alpha=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('P_mid (uM)')\n",
    "plt.title('P_mid_Averaged with STD error vs T_der')\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29463ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Remove commenting out for plotting\\n#The part below is a visual check(plotting [A]t with error vs time)\\nplt.figure(figsize=(10, 6))\\nfor i in range(A.shape[0]):\\n        plt.plot(T_der, A[i, :], label=f'{rounded_conc[i]} uM of subst.')\\n        plt.plot(T_der, A[i, :]-A_std[i, :], alpha=0.3)\\n        plt.plot(T_der, A[i, :]+A_std[i, :], alpha=0.3)\\nplt.xlabel('Time (s)')\\nplt.ylabel('[A]t (uM)')\\nplt.title('[A]t vs T_der')\\nplt.legend(fontsize='x-small')\\nplt.grid(True)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = dPdt_averaged/kI #Finding I \n",
    "I_std = I * np.sqrt((dPdt_std/dPdt_averaged)**2 + (kI_SD/kI)**2) #Doing error propagation on I\n",
    "\n",
    "'''\n",
    "#Remove commenting out for plotting\n",
    "#The part below is a visual check(plotting [I]t with error vs time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(I.shape[0]):\n",
    "        plt.plot(T_der, I[i, :], label=f'{rounded_conc[i]} uM of subst.')\n",
    "        plt.plot(T_der, I[i, :]-I_std[i, :], alpha=0.3)\n",
    "        plt.plot(T_der, I[i, :]+I_std[i, :], alpha=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('[I]t (uM)')\n",
    "plt.title('[I]t Averaged vs T_der')\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.show()\n",
    "'''\n",
    "A = I + P_mid_averaged #Finding A\n",
    "A_std = np.sqrt((I_std)**2+(P_mid_std)**2) #Doing error propagation on A\n",
    "\n",
    "'''\n",
    "#Remove commenting out for plotting\n",
    "#The part below is a visual check(plotting [A]t with error vs time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(A.shape[0]):\n",
    "        plt.plot(T_der, A[i, :], label=f'{rounded_conc[i]} uM of subst.')\n",
    "        plt.plot(T_der, A[i, :]-A_std[i, :], alpha=0.3)\n",
    "        plt.plot(T_der, A[i, :]+A_std[i, :], alpha=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('[A]t (uM)')\n",
    "plt.title('[A]t vs T_der')\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "529ffb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting off beyond 88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#Remove commenting out for plotting\\n#The part below is a visual check(plotting shortened_[A]t with error vs time)\\nplt.figure(figsize=(10, 6))\\nfor i in range(shortened_A.shape[0]):\\n        plt.plot(shortened_T_der, shortened_A[i, :],label=f'{rounded_conc[i]} uM of subst.')\\n        plt.plot(shortened_T_der, shortened_A[i, :]-shortened_A_std[i, :], alpha=0.3)\\n        plt.plot(shortened_T_der, shortened_A[i, :]+shortened_A_std[i, :], alpha=0.3)\\nplt.xlabel('Time (s)')\\nplt.ylabel('Shortened [A]t (uM)')\\nplt.title('Shortened [A]t vs T_der')\\nplt.legend(fontsize='x-small')\\nplt.grid(True)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, need to ensure that time points of A used for fitting are less than 10% of [S]0\n",
    "#This is a convention (initial rates are determined in the first 10% of substrate conversion)\n",
    "\n",
    "thresh_sub_conc = 0.1*np.array(Sub_conc) #10% threshold of [S]0\n",
    "first_larger_indices = []\n",
    "\n",
    "# Iterate over each column and its corresponding concentration and find the \"cutoff\" index\n",
    "for idx, concentration in enumerate(thresh_sub_conc):\n",
    "    column = A[idx, :]  # Get the column from A\n",
    "    # Find indices where values are larger than the concentration threshold \n",
    "    larger_indices = np.where(column > concentration)[0]\n",
    "    # Get the first index if any are found, otherwise return None\n",
    "    first_index = larger_indices[0] if larger_indices.size > 0 else None\n",
    "    first_larger_indices.append(first_index)\n",
    "    \n",
    "filtered_first_larger_indices = [x for x in first_larger_indices if x is not None]\n",
    "cutoff = min(filtered_first_larger_indices)\n",
    "\n",
    "#Shorten the A and T_der datasets to be within 10% of substrate conversion\n",
    "shortened_A = A[:,:cutoff]\n",
    "shortened_A_std = A_std[:,:cutoff]\n",
    "shortened_T_der = T_der[:cutoff]\n",
    "\n",
    "print(\"cutting off beyond\", cutoff)\n",
    "\n",
    "#Exporting shortened A values, STD values, and shortned T_der into Excel for plotting \n",
    "shortened_A_df = pandas.DataFrame(shortened_A)\n",
    "shortened_A_std_df = pandas.DataFrame(shortened_A_std)\n",
    "shortened_T_der_df = pandas.DataFrame(shortened_T_der)\n",
    "\n",
    "with pandas.ExcelWriter('OUTPUT_kcat_kM_experiment_Opt-2_shortened_A_std_and_T_der.xlsx', engine='openpyxl') as writer:\n",
    "    shortened_A_df.to_excel(writer, sheet_name='shortened_A')\n",
    "    shortened_A_std_df.to_excel(writer, sheet_name='shortened_A_std')\n",
    "    shortened_T_der_df.to_excel(writer, sheet_name='shortened_T_der')\n",
    "\n",
    "'''\n",
    "#Remove commenting out for plotting\n",
    "#The part below is a visual check(plotting shortened_[A]t with error vs time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(shortened_A.shape[0]):\n",
    "        plt.plot(shortened_T_der, shortened_A[i, :],label=f'{rounded_conc[i]} uM of subst.')\n",
    "        plt.plot(shortened_T_der, shortened_A[i, :]-shortened_A_std[i, :], alpha=0.3)\n",
    "        plt.plot(shortened_T_der, shortened_A[i, :]+shortened_A_std[i, :], alpha=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Shortened [A]t (uM)')\n",
    "plt.title('Shortened [A]t vs T_der')\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02f57849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Remove commenting out for plotting\\n#The part below is a visual check(plotting shortened_[A]t with error vs time)\\nplt.plot(Sub_conc,dAdt,'r.',markersize=15)\\nplt.plot(Sub_conc,dAdt+dAdt_SD,'b-',alpha=0.2)\\nplt.plot(Sub_conc,dAdt-dAdt_SD,'b-',alpha=0.2)\\nplt.xlabel('Concentration (uM)')\\nplt.ylabel('dAdt (uM s-1)')\\nplt.title('dAdt vs Concentration')\\nplt.legend(fontsize='x-small')\\nplt.grid(True)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next, the slopes of shortened_A vs shortened_T_der graphs need to be determined (dAdt)\n",
    "#Those slopes correspond to rates in the y-axis of traditional Michaelis-Menten plots\n",
    "#In order for curve_fit to determine slopes correctly, the diagnoal values of the covariance matrix\n",
    "#need to be smaller than the off-diagonal values (this also help verify that the fitting model is correct)\n",
    "#Note that time scales are much larger than concentration scales, which can lead to numerical instability\n",
    "#To stably apply curve_fit, we first normalize shortened_T_der and shortened_A and then convert them back\n",
    "#To the normal scale after fitting\n",
    "#The applied normalization ensures that the mean of the datasets is 0 and the standard deviation is 1 \n",
    "\n",
    "shortened_T_der_norm = (shortened_T_der-np.mean(shortened_T_der))/np.std(shortened_T_der)\n",
    "shortened_A_norm = (shortened_A-np.mean(shortened_A))/np.std(shortened_A)\n",
    "shortened_A_norm_std = shortened_A_std/np.std(shortened_A_std)\n",
    "\n",
    "\n",
    "#Fit to a straight line \n",
    "def linear_model(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "dAdt = []\n",
    "dAdt_SD = []\n",
    "\n",
    "for i in range(8):\n",
    "    popt, pcov = curve_fit(linear_model, shortened_T_der_norm, shortened_A_norm[i], sigma=shortened_A_norm_std[i], absolute_sigma=True)\n",
    "    a, b = popt\n",
    "    #The standard deviation of the slope is the square root of the top left most value in the covariance matrix\n",
    "    std_errors = np.sqrt(pcov[0,0])\n",
    "    dAdt.append(a)\n",
    "    dAdt_SD.append(std_errors)\n",
    "\n",
    "# Rescale the obtain values back to their original scale\n",
    "factor = np.std(shortened_A)/np.std(shortened_T_der)\n",
    "dAdt = np.array(dAdt)*factor\n",
    "dAdt_SD = np.array(dAdt_SD)*factor\n",
    "\n",
    "'''\n",
    "#Remove commenting out for plotting\n",
    "#The part below is a visual check(plotting shortened_[A]t with error vs time)\n",
    "plt.plot(Sub_conc,dAdt,'r.',markersize=15)\n",
    "plt.plot(Sub_conc,dAdt+dAdt_SD,'b-',alpha=0.2)\n",
    "plt.plot(Sub_conc,dAdt-dAdt_SD,'b-',alpha=0.2)\n",
    "plt.xlabel('Concentration (uM)')\n",
    "plt.ylabel('dAdt (uM s-1)')\n",
    "plt.title('dAdt vs Concentration')\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48914d6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the slope (in s-1) = 4.083490244556711e-05\n",
      "Value of b (in M s-1) = -1.4809525610675774e-17\n",
      "Kcat/Km = 1633.3960978226844\n",
      "Kcat/Km 95% confidence interval = 315.33777891298354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "#Apply curve_fit with the same scaling/resclaing to find the slope of the graph where y = dAdt and x = Substrate concentration\n",
    "#In the right units, slope = kcat/KM * [E]0 where [E]0 = initial enzyme concentration\n",
    "Sub_conc_norm = (Sub_conc-np.mean(Sub_conc))/np.std(Sub_conc)\n",
    "dAdt_norm = (dAdt-np.mean(dAdt))/np.std(dAdt)\n",
    "dAdt_norm_std = dAdt_SD/np.std(dAdt)\n",
    "\n",
    "popt, pcov = curve_fit(linear_model, Sub_conc_norm, dAdt_norm, sigma=dAdt_norm_std, absolute_sigma=True)\n",
    "a, b = popt\n",
    "std_errors = np.sqrt(pcov[0,0])\n",
    "#print(pcov) to check that the diagonal values of the matrix are larger than the off-diagonal ones\n",
    "\n",
    "#Rescale the values\n",
    "factor = np.std(dAdt)/np.std(Sub_conc)\n",
    "slope = a*factor #in units of s-1\n",
    "b = b*factor #in units of uM/s\n",
    "slope_SD = std_errors*factor #in units of s-1\n",
    "\n",
    "#Convert from units of uM to units of M\n",
    "b_M = b*factor/(10**6)\n",
    "\n",
    "#Printing slope and b where y = slope*x + b\n",
    "print(\"Value of the slope (in s-1) =\", slope)\n",
    "print(\"Value of b (in M s-1) =\", b_M)\n",
    "\n",
    "#To find kcat over Km, use the formula slope = kcat/KM*[E]0\n",
    "#[E]0 used in this experiment was 25 nM, which is 25/(10**9) M\n",
    "kcat_over_Km = slope/(25/(10**9)) # divided by [E]0 in M\n",
    "kcat_over_Km_SD = slope_SD/(25/(10**9)) #standard deviation is 68% confidence interval\n",
    "kcat_over_Km_95_conf_int = kcat_over_Km_SD*1.96\n",
    "\n",
    "#Final values of Kcat/Km and the 95% confidence interval\n",
    "print(\"Kcat/Km =\", kcat_over_Km)\n",
    "print(\"Kcat/Km 95% confidence interval =\", kcat_over_Km_95_conf_int)\n",
    "\n",
    "#Converting dAdt, dAdt_SD, and Concentrations from units of uM/s to units of M/s\n",
    "dAdt_M = np.array(dAdt)/(10**6)\n",
    "dAdt_SD_M = np.array(dAdt_SD)/(10**6)\n",
    "Sub_conc_M = np.array(Sub_conc)/(10**6)\n",
    "\n",
    "#Also, generate y values using the fitted linear equation for plotting the fit\n",
    "y = []\n",
    "\n",
    "for i in range(8):\n",
    "    y_value = slope*Sub_conc_M[i]+b_M\n",
    "    y.append(y_value)\n",
    "\n",
    "#Exporting dAdt_M values, STD values on dAdt_M, and Concentrations_M values into Excel for plotting \n",
    "dAdt_df = pandas.DataFrame(dAdt_M)\n",
    "dAdt_SD_df = pandas.DataFrame(dAdt_SD_M)\n",
    "Sub_conc_df = pandas.DataFrame(Sub_conc_M)\n",
    "y_df = pandas.DataFrame(y)\n",
    "kcat_over_Km_df = pandas.DataFrame([kcat_over_Km])\n",
    "kcat_over_Km_95_conf_int_df = pandas.DataFrame([kcat_over_Km_95_conf_int])\n",
    "b_M_df = pandas.DataFrame([b_M])\n",
    "slope_df = pandas.DataFrame([slope])\n",
    "\n",
    "with pandas.ExcelWriter('OUTPUT_kcat_kM_experiment_Opt-2_dAdt_std_Concentrations_fitted_y_values_Kcat_Km_error_slope_b_int.xlsx', engine='openpyxl') as writer:\n",
    "    dAdt_df.to_excel(writer, sheet_name='dAdt')\n",
    "    dAdt_SD_df.to_excel(writer, sheet_name='dAdt_std')\n",
    "    Sub_conc_df.to_excel(writer, sheet_name='Concentrations')\n",
    "    y_df.to_excel(writer, sheet_name='Fitted y')\n",
    "    kcat_over_Km_df.to_excel(writer, sheet_name='kcat over Km')\n",
    "    kcat_over_Km_95_conf_int_df.to_excel(writer, sheet_name='kcat over Km 95% conf inter')\n",
    "    b_M_df.to_excel(writer, sheet_name='b_M')\n",
    "    slope_df.to_excel(writer, sheet_name='slope')\n",
    "\n",
    "plt.plot(Sub_conc_M,dAdt_M,'r.',markersize=15)\n",
    "plt.plot(Sub_conc_M,dAdt_M+dAdt_SD_M,'k.',alpha=0.2)\n",
    "plt.plot(Sub_conc_M,dAdt_M-dAdt_SD_M,'k.',alpha=0.2)\n",
    "plt.plot(Sub_conc_M,dAdt_M+dAdt_SD_M,'b-',alpha=0.2)\n",
    "plt.plot(Sub_conc_M,dAdt_M-dAdt_SD_M,'b-',alpha=0.2)\n",
    "plt.plot(Sub_conc_M,slope*np.array(Sub_conc_M)+b_M,'k--')\n",
    "plt.xlabel('Concentration (M)')\n",
    "plt.ylabel('dAdt (M s-1)')\n",
    "plt.title('dAdt vs Concentration')\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c580fc",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2eaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004f06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
